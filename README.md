# Exploring Reinforcement Learning

The aim of this project is simply exploring different techniques for reinforcement learning.
To this end, I adapted an implementation of a maze from [Nima Siboni](https://github.com/nima-siboni), who has given an [amazing lecture](https://github.com/nima-siboni/RL-course-batch-25-DSR) at Data Science Retreat.

At the current state the environment is either composed of a Maze generated by the Prim algorithm, or set manually at start time using a simple GUI.
The agent then starts to learn to navigate the specific maze using q-learning either in a greedy or softmax-fashion.

### Exploring the maze
The agent is tasked to navigate a randomly-generated maze and arrive at the bottom right corner (red). To do this the agent needs to bypass obstacles (black) and tries to minimize the punishment for arriving late. 

| Episodes = 50 | Episodes = 350 |
| --- | --- |
|<image src="data/run_episode_50.gif" width="500" height="500"/>| <image src="data/run_episode_350.gif" width="500" height="500"/>|

### Learning the value function
In a traditional q-learning fashion, the policy is learned by approximating a value function and derive appropriate quality of actions (indicated by the size of the arrows), from which random actions are chosen at each step in time. The evolution of the value function is shown during training:


<p align="center">
  <image src="data/value_evolution_prim_maze.gif" width="500" height="500"/>
</p>
  
The colormap indicates the magnitude of the value function at the different states, where yellow is high value and purple is low value.

### Navigating Custom Mazes
This setup allows to learn virtually any maze geometry, which can be explored by using a simple GUI where the obstacles can be provided manually at the beginning of the training. Here, is an example

<p align="center">
  <image src="data/run_episode_1500_custom_maze.gif" width="350" height="350"/>
</p>

## Setup

To run this code, you should consider creating a new python (3.8) environment (venv, conda, ...) and then execute:

```bash
pip install -r requirements
```

This will install all the models that you should need to run the tested parts of this repository.

## Run Training

To run the training simply call the Makefile:

```bash
make train-instruction INSTRUCTION=<path to instruction, e.g. "instructions/prim_maze.json">
```

By default this will run the training of an agent, displaying the value maps and simulations in regular intervals.


